### **Total: 35 Questions**

---

### **SN10-1 (FP)**

What is the main objective of the backpropagation algorithm?

A) To randomly initialize network weights
B) To propagate inputs forward through the network
C) To minimize the error function by adjusting weights
D) To eliminate hidden layers

**Correct Answer:** C

---

### **SN10-2 (FP)**

Why must activation functions be differentiable in backpropagation?

A) To speed up computation
B) To apply the chain rule of calculus
C) To reduce overfitting
D) To simplify network architecture

**Correct Answer:** B

---

### **SN10-3 (FP)**

Which limitation of the perceptron model led to the development of multi-layer neural networks?

A) Inability to process large datasets
B) Inability to learn non-linearly separable functions
C) High computational cost
D) Sensitivity to noise

**Correct Answer:** B

---

### **SN10-4 (FP)**

Which problem is classically used to demonstrate the failure of single-layer perceptrons?

A) AND
B) OR
C) XOR
D) NAND

**Correct Answer:** C

---

### **SN10-5 (FP)**

Backpropagation is primarily applied to which type of neural network?

A) Recurrent neural networks
B) Feedforward neural networks
C) Bayesian networks
D) Hopfield networks

**Correct Answer:** B

---

### **SN10-6 (FP)**

What role does the error (loss) function play in backpropagation?

A) It defines network topology
B) It measures the difference between predicted and target outputs
C) It initializes weights
D) It selects input features

**Correct Answer:** B

---

### **SN10-7 (FP)**

Which loss function is commonly used in classical backpropagation examples in the slides?

A) Cross-entropy loss
B) Hinge loss
C) Sum of squared errors
D) KL-divergence

**Correct Answer:** C

---

### **SN10-8 (FP)**

What does gradient descent attempt to optimize during training?

A) Number of neurons
B) Network depth
C) Error function
D) Input dimensionality

**Correct Answer:** C

---

### **SN10-9**

What is the most likely consequence of choosing a very large learning rate?

A) Very slow convergence
B) No weight updates
C) Oscillation or divergence during training
D) Guaranteed global minimum

**Correct Answer:** C

---

### **SN10-10 (FP)**

What happens if the learning rate is chosen too small?

A) Training becomes unstable
B) Training converges very slowly
C) The model overfits immediately
D) Gradients explode

**Correct Answer:** B

---

### **SN10-11 (FP)**

Which mathematical principle enables error propagation from output to hidden layers?

A) Linear algebra
B) Probability theory
C) Chain rule
D) Bayesian inference

**Correct Answer:** C

---

### **SN10-12**

Which layer directly computes the output error in backpropagation?

A) Input layer
B) First hidden layer
C) Output layer
D) All layers simultaneously

**Correct Answer:** C

---

### **SN10-13**

In backpropagation, how are hidden layer errors computed?

A) Randomly
B) From the output layer errors
C) From the input data
D) From the bias values

**Correct Answer:** B

---

### **SN10-14 (FP)**

Why are non-differentiable activation functions problematic for backpropagation?

A) They increase memory usage
B) Gradients cannot be computed
C) They slow down inference
D) They require more data

**Correct Answer:** B

---

### **SN10-15**

Which activation function was historically popular but can cause vanishing gradients?

A) ReLU
B) Linear
C) Sigmoid
D) Softmax

**Correct Answer:** C

---

### **SN10-16**

What does the vanishing gradient problem mainly affect?

A) Input normalization
B) Deep neural networks
C) Linear classifiers
D) Output layer only

**Correct Answer:** B

---

### **SN10-17 (FP)**

Why is differentiability essential for gradient-based learning?

A) It reduces noise
B) It allows analytical gradient computation
C) It simplifies datasets
D) It guarantees optimality

**Correct Answer:** B

---

### **SN10-18**

Which component controls the step size of weight updates?

A) Bias
B) Momentum
C) Learning rate
D) Activation function

**Correct Answer:** C

---

### **SN10-19 (FP)**

What is the main advantage of multi-layer neural networks over perceptrons?

A) Faster training
B) Lower memory usage
C) Ability to model nonlinear decision boundaries
D) Simpler mathematical formulation

**Correct Answer:** C

---

### **SN10-20**

Which optimization problem does neural network training correspond to?

A) Sorting
B) Search
C) Function minimization
D) Clustering

**Correct Answer:** C

---

### **SN10-21 (FP)**

Which classifier aims to maximize the margin between classes?

A) Perceptron
B) Naive Bayes
C) Support Vector Machine
D) k-NN

**Correct Answer:** C

---

### **SN10-22 (FP)**

What is the core idea behind the kernel trick in SVMs?

A) Reducing dimensionality
B) Mapping data to higher-dimensional space
C) Eliminating support vectors
D) Avoiding optimization

**Correct Answer:** B

---

### **SN10-23 (FP)**

Why does maximizing the margin improve generalization in SVMs?

A) It reduces training time
B) It increases model complexity
C) It reduces sensitivity to noise
D) It eliminates outliers

**Correct Answer:** C

---

### **SN10-24**

Which kernel is commonly used for non-linear classification?

A) Linear
B) Polynomial
C) Radial Basis Function (RBF)
D) Identity

**Correct Answer:** C

---

### **SN10-25**

What do support vectors represent?

A) All training samples
B) Misclassified samples
C) Boundary-defining samples
D) Random points

**Correct Answer:** C

---

### **SN10-26**

Which statement best describes backpropagation?

A) A rule-based inference method
B) A supervised learning algorithm
C) An unsupervised clustering method
D) A symbolic reasoning system

**Correct Answer:** B

---

### **SN10-27 (FP)**

Why is backpropagation considered a breakthrough in neural network history?

A) It eliminated the need for data
B) It enabled training of multi-layer networks
C) It replaced symbolic AI
D) It guarantees global optimality

**Correct Answer:** B

---

### **SN10-28**

Which factor most strongly affects convergence speed?

A) Number of classes
B) Learning rate
C) Dataset size
D) Output labels

**Correct Answer:** B

---

### **SN10-29**

What does a local minimum represent in training?

A) Best possible solution
B) A point where gradients are zero but not global optimum
C) Model failure
D) Overfitting state

**Correct Answer:** B

---

### **SN10-30**

Why can training get stuck in local minima?

A) Discrete weights
B) Non-convex error surfaces
C) Linear activation functions
D) Small datasets

**Correct Answer:** B

---

### **SN10-31**

Which method helps reduce oscillations during gradient descent?

A) Feature scaling
B) Momentum
C) Dropout
D) Regularization

**Correct Answer:** B

---

### **SN10-32**

What is the role of bias terms in neural networks?

A) Increase input size
B) Shift activation thresholds
C) Normalize gradients
D) Reduce overfitting

**Correct Answer:** B

---

### **SN10-33**

Which statement about SVMs is correct?

A) They minimize squared error
B) They maximize classification margin
C) They require differentiable activation functions
D) They are unsupervised

**Correct Answer:** B

---

### **SN10-34**

Compared to perceptrons, SVMs primarily differ in:

A) Use of kernels and margins
B) Requirement of labeled data
C) Binary output
D) Linear decision boundaries only

**Correct Answer:** A

---

### **SN10-35**

Which learning paradigm do both backpropagation and SVMs belong to?

A) Unsupervised learning
B) Reinforcement learning
C) Supervised learning
D) Evolutionary learning

**Correct Answer:** C

---