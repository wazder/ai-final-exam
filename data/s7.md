### **Total: 25 Questions**

---

### **SN7-1 (FP)**

What is the fundamental assumption of the Naive Bayes classifier?

A) Features are linearly separable
B) Features are conditionally independent given the class
C) Classes have equal prior probabilities
D) Training data is noise-free

**Correct Answer:** B

---

### **SN7-2 (FP)**

Why can Naive Bayes still perform well even when the independence assumption is violated?

A) It uses deep architectures
B) Errors often cancel out in probability estimation
C) It ignores irrelevant features automatically
D) It relies on rule-based inference

**Correct Answer:** B

---

### **SN7-3 (FP)**

Which probability does Naive Bayes compute to perform classification?

A) P(Features)
B) P(Class | Features)
C) P(Features | Features)
D) P(Class | Class)

**Correct Answer:** B

---

### **SN7-4 (FP)**

Which theorem provides the mathematical foundation for Naive Bayes?

A) Central Limit Theorem
B) Bayes’ Theorem
C) Law of Large Numbers
D) Chain Rule of Calculus

**Correct Answer:** B

---

### **SN7-5 (FP)**

What is a key limitation of the perceptron model?

A) It requires too much memory
B) It can only learn linearly separable patterns
C) It cannot handle numerical data
D) It is unsupervised

**Correct Answer:** B

---

### **SN7-6 (FP)**

How do multi-layer neural networks overcome the perceptron’s limitation?

A) By using symbolic rules
B) By introducing hidden layers
C) By reducing input features
D) By removing activation functions

**Correct Answer:** B

---

### **SN7-7 (FP)**

What type of learning problem does a decision tree primarily solve?

A) Unsupervised clustering
B) Reinforcement learning
C) Supervised classification
D) Sequence prediction

**Correct Answer:** C

---

### **SN7-8 (FP)**

Which criterion is commonly used to split nodes in decision tree learning?

A) Euclidean distance
B) Information gain
C) Learning rate
D) Margin size

**Correct Answer:** B

---

### **SN7-9 (FP)**

What is a major advantage of decision trees?

A) They require large datasets
B) They are easy to interpret
C) They are always optimal
D) They avoid overfitting automatically

**Correct Answer:** B

---

### **SN7-10 (FP)**

Which problem do decision trees commonly suffer from?

A) Underfitting only
B) Overfitting
C) Lack of interpretability
D) Requirement of kernels

**Correct Answer:** B

---

### **SN7-11**

What technique is commonly used to reduce overfitting in decision trees?

A) Increasing depth
B) Pruning
C) Increasing learning rate
D) Removing labels

**Correct Answer:** B

---

### **SN7-12 (FP)**

What does correlation analysis help identify in datasets?

A) Causal relationships only
B) Feature relationships and dependencies
C) Optimal classifiers
D) Network architectures

**Correct Answer:** B

---

### **SN7-13 (FP)**

Why is feature selection important in machine learning?

A) It increases dataset size
B) It improves model accuracy and efficiency
C) It removes the need for training
D) It guarantees generalization

**Correct Answer:** B

---

### **SN7-14 (FP)**

Which method classifies a data point based on similarity to stored examples?

A) Decision trees
B) Naive Bayes
C) Nearest Neighbor
D) Perceptron

**Correct Answer:** C

---

### **SN7-15 (FP)**

What is a key drawback of the k-Nearest Neighbor method?

A) It requires model training
B) It has high computation cost at prediction time
C) It assumes feature independence
D) It cannot handle noise

**Correct Answer:** B

---

### **SN7-16 (FP)**

Which distance metric is commonly used in nearest neighbor methods?

A) Manhattan distance
B) Euclidean distance
C) Hamming distance
D) All of the above

**Correct Answer:** D

---

### **SN7-17**

What happens when k is chosen too small in k-NN?

A) Model underfits
B) Model overfits
C) Computation becomes impossible
D) Distance metrics fail

**Correct Answer:** B

---

### **SN7-18**

What happens when k is chosen too large in k-NN?

A) Overfitting increases
B) Decision boundaries become smoother
C) Training fails
D) Noise dominates

**Correct Answer:** B

---

### **SN7-19 (FP)**

Which learning paradigm does Naive Bayes belong to?

A) Unsupervised learning
B) Supervised learning
C) Reinforcement learning
D) Evolutionary learning

**Correct Answer:** B

---

### **SN7-20 (FP)**

Which statement about probabilistic classifiers is correct?

A) They output only class labels
B) They ignore uncertainty
C) They model uncertainty explicitly
D) They require linear separability

**Correct Answer:** C

---

### **SN7-21**

Which factor most influences decision tree complexity?

A) Learning rate
B) Tree depth
C) Distance metric
D) Prior probabilities

**Correct Answer:** B

---

### **SN7-22 (FP)**

Why are ensemble methods like Random Forests effective?

A) They use a single deep tree
B) They combine multiple weak learners
C) They eliminate randomness
D) They avoid training

**Correct Answer:** B

---

### **SN7-23**

Which concept helps evaluate relationships between numerical features?

A) Entropy
B) Correlation coefficient
C) Kernel trick
D) Margin

**Correct Answer:** B

---

### **SN7-24**

Which classifier is most sensitive to irrelevant features?

A) Naive Bayes
B) k-Nearest Neighbor
C) Decision Tree
D) SVM

**Correct Answer:** B

---

### **SN7-25**

Which statement best summarizes the goal of classification algorithms?

A) Discover hidden clusters
B) Predict continuous values
C) Assign labels to unseen data
D) Optimize memory usage

**Correct Answer:** C

---